{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.75, device=0) \n",
    "    \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 64\n",
    "N_EPISODES = 500\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_DIR = \"../src/train_denoised/\"\n",
    "TEST_DIR = \"../src/test_denoised/\"\n",
    "MODEL_SAVE_PATH = \"../models/ppo_rnn_model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Структура модели & обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.files = [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "        self.data = []\n",
    "\n",
    "        for file_path in self.files:\n",
    "            df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "            if df.empty or \"pressure\" not in df:\n",
    "                continue\n",
    "            \n",
    "            for i in range(len(df) - SEQUENCE_LENGTH):\n",
    "                self.data.append(df[\"pressure\"].iloc[i: i + SEQUENCE_LENGTH].values.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(TRAIN_DIR)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, hidden_dim=128):\n",
    "        super(RNNFeatureExtractor, self).__init__(observation_space, features_dim=hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return x\n",
    "\n",
    "class TimeSeriesEnv(gym.Env):\n",
    "    def __init__(self, dataloader):\n",
    "        super(TimeSeriesEnv, self).__init__()\n",
    "        self.dataloader = iter(dataloader)\n",
    "        self.current_batch = next(self.dataloader)\n",
    "        self.batch_idx = 0  \n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(SEQUENCE_LENGTH, 1), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  \n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            self.current_batch = next(self.dataloader)  \n",
    "        except StopIteration:\n",
    "            self.dataloader = iter(train_dataloader)  \n",
    "            self.current_batch = next(self.dataloader)\n",
    "\n",
    "        self.batch_idx = 0  \n",
    "        return self.current_batch[self.batch_idx].reshape(SEQUENCE_LENGTH, 1).cpu().numpy() \n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.uniform(-1, 1)  \n",
    "        self.batch_idx += 1  \n",
    "\n",
    "        if self.batch_idx >= BATCH_SIZE:  \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        obs = self.current_batch[self.batch_idx].reshape(SEQUENCE_LENGTH, 1).cpu().numpy() if not done else self.reset() \n",
    "        return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 430  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 324           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5591842e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -0.00586      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.24          |\n",
      "|    n_updates            | 3             |\n",
      "|    policy_gradient_loss | 7.58e-05      |\n",
      "|    value_loss           | 0.65          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 298         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003402509 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.00208    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.132       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 0.341       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 288          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015142066 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.000145     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.182        |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | -0.000652    |\n",
      "|    value_loss           | 0.377        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 284           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 36            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3948134e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -0.00323      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.183         |\n",
      "|    n_updates            | 12            |\n",
      "|    policy_gradient_loss | 5.61e-05      |\n",
      "|    value_loss           | 0.351         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002596982 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.00207     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.144        |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.000373    |\n",
      "|    value_loss           | 0.311        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m     10\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m callback \u001b[38;5;241m=\u001b[39m StopTrainingOnMaxEpisodes(max_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPISODES\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/policies.py:654\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[1;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m--> 654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/policies.py:697\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, MultiCategoricalDistribution):\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/distributions.py:288\u001b[0m, in \u001b[0;36mCategoricalDistribution.proba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfCategoricalDistribution:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_env = DummyVecEnv([lambda: TimeSeriesEnv(train_dataloader)])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=RNNFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(hidden_dim=128),\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_env,\n",
    "    learning_rate=2.5e-4, \n",
    "    n_epochs=3,\n",
    "    ent_coef=0.01,  \n",
    "    target_kl=0.005,  \n",
    "    clip_range=0.15,\n",
    "    verbose=1,\n",
    "    device=device,\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "callback = StopTrainingOnMaxEpisodes(max_episodes=1000, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=N_EPISODES * len(train_dataloader), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание, формирование submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_INPUT_PATH = \"../src/raw_data/submit.csv\"\n",
    "SUBMIT_OUTPUT_PATH = \"../src/submit.csv\"\n",
    "MODEL_SAVE_PATH = \"../models/ppo_transformer_model.pkl\"\n",
    "TEST_DIR = \"../src/test_denoised/\"\n",
    "SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PPO.load(MODEL_SAVE_PATH, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intervals(predictions, time_series):\n",
    "    intervals = []\n",
    "    start = None\n",
    "\n",
    "    for i in range(len(predictions) - 1):\n",
    "        if predictions[i] == predictions[i + 1]:  \n",
    "            if start is None:  \n",
    "                start = time_series[i]\n",
    "        else:\n",
    "            if start is not None:\n",
    "                end = time_series[i]\n",
    "                intervals.append([start, end])\n",
    "                start = None  \n",
    "\n",
    "    if start is not None:\n",
    "        intervals.append([start, time_series[-1]])\n",
    "\n",
    "    return intervals\n",
    "\n",
    "submit_df = pd.read_csv(SUBMIT_INPUT_PATH, names=[\"file\", \"recovery\", \"drop\"])\n",
    "\n",
    "predicted_dfs = []\n",
    "results = []\n",
    "\n",
    "for _, row in submit_df.iterrows():\n",
    "    file_name = row[\"file\"]\n",
    "    file_path = os.path.join(TEST_DIR, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_name} not found\")\n",
    "        results.append([file_name, [], []])\n",
    "        continue\n",
    "\n",
    "    df_test = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "\n",
    "    test_sequences = np.array([\n",
    "        df_test[\"pressure\"].iloc[i: i + SEQUENCE_LENGTH].values.reshape(SEQUENCE_LENGTH, 1)\n",
    "        for i in range(len(df_test) - SEQUENCE_LENGTH)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    all_predictions = [[] for _ in range(len(df_test))]\n",
    "\n",
    "    for start in range(len(test_sequences)):\n",
    "        obs = test_sequences[start][None, :, :] \n",
    "        pred_class, _ = loaded_model.predict(obs, deterministic=True)\n",
    "\n",
    "        for i in range(SEQUENCE_LENGTH):\n",
    "            if start + i < len(df_test):\n",
    "                all_predictions[start + i].append(pred_class)\n",
    "\n",
    "\n",
    "    final_classes = np.zeros(len(df_test))\n",
    "\n",
    "    for i in range(len(all_predictions)):\n",
    "        if all_predictions[i]:  \n",
    "            final_classes[i] = Counter(all_predictions[i]).most_common(1)[0][0]  \n",
    "        elif i > 0:  \n",
    "            final_classes[i] = final_classes[i - 1]  \n",
    "\n",
    "    df_test[\"prediction\"] = final_classes\n",
    "    predicted_dfs.append(df_test)\n",
    "\n",
    "    recovery_intervals = extract_intervals(final_classes == 2, df_test[\"time\"].values)\n",
    "    drop_intervals = extract_intervals(final_classes == 1, df_test[\"time\"].values)\n",
    "\n",
    "    results.append([file_name, recovery_intervals, drop_intervals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_output_df = pd.DataFrame(results, columns=[\"file\", \"recovery\", \"drop\"])\n",
    "submit_output_df.to_csv(SUBMIT_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка предсказаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "дальше нужно проанализировать predicted_dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
