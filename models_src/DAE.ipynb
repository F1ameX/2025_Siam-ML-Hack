{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../src/raw_data')\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>mark</th>\n",
       "      <th>recovery</th>\n",
       "      <th>drop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00e03657-8e1e-4c8c-a724-1d3c77b48510</td>\n",
       "      <td>[0.0,235.9225,237.06666666666666,2076.06055555...</td>\n",
       "      <td>[[2419.9805555555554,2437.4241666666667],[3177...</td>\n",
       "      <td>[[3453.6875,3763.9605555555554]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00e4dba2-36d2-42b4-beb1-c55aed75f506</td>\n",
       "      <td>[0.0,7979.234444444444,13284.465,19439.8005555...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[13284.465,19439.800555555557]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00f035b7-ad7a-4f30-9081-522a3c10805b</td>\n",
       "      <td>[0.0,42.75,2438.3330555555553]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0,42.75]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01a0c034-6afc-4e73-95fa-621f702a0b7d</td>\n",
       "      <td>[0.0,491.98305555555555,1439.9830555555557,154...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0,491.98305555555555]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01a530d3-6496-4515-9fbb-4f44e298fd29</td>\n",
       "      <td>[0.0,1287.0341666666666,1288.0483333333334,156...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[4920.376666666667,6208.231666666667]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1dfaf03c-e297-4d92-a0bf-40b1a829391f</td>\n",
       "      <td>[0.0,7.4,7.933055555555556,14.466666666666667,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1e149fbd-41c6-4779-b87d-c5dc17fbb4c0</td>\n",
       "      <td>[0.0,635.3127777777778]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0,635.3127777777778]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1e19b77c-8a0e-4749-a384-9c1e679035bf</td>\n",
       "      <td>[0.0,82.16555555555556,216.66027777777776,229....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1e4b4c18-1e32-45eb-917a-5760e33fbaca</td>\n",
       "      <td>[0.0,1217.8258333333333,1223.6030555555556,125...</td>\n",
       "      <td>[[9541.77638888889,10288.5075]]</td>\n",
       "      <td>[[10339.343055555555,10739.613055555556],[1311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1e7f7ecb-a6a7-40ef-9d1c-48aa96eb6c38</td>\n",
       "      <td>[0.0,12.016666666666667,21.483055555555556,640...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file  \\\n",
       "0   00e03657-8e1e-4c8c-a724-1d3c77b48510   \n",
       "1   00e4dba2-36d2-42b4-beb1-c55aed75f506   \n",
       "2   00f035b7-ad7a-4f30-9081-522a3c10805b   \n",
       "3   01a0c034-6afc-4e73-95fa-621f702a0b7d   \n",
       "4   01a530d3-6496-4515-9fbb-4f44e298fd29   \n",
       "..                                   ...   \n",
       "95  1dfaf03c-e297-4d92-a0bf-40b1a829391f   \n",
       "96  1e149fbd-41c6-4779-b87d-c5dc17fbb4c0   \n",
       "97  1e19b77c-8a0e-4749-a384-9c1e679035bf   \n",
       "98  1e4b4c18-1e32-45eb-917a-5760e33fbaca   \n",
       "99  1e7f7ecb-a6a7-40ef-9d1c-48aa96eb6c38   \n",
       "\n",
       "                                                 mark  \\\n",
       "0   [0.0,235.9225,237.06666666666666,2076.06055555...   \n",
       "1   [0.0,7979.234444444444,13284.465,19439.8005555...   \n",
       "2                      [0.0,42.75,2438.3330555555553]   \n",
       "3   [0.0,491.98305555555555,1439.9830555555557,154...   \n",
       "4   [0.0,1287.0341666666666,1288.0483333333334,156...   \n",
       "..                                                ...   \n",
       "95  [0.0,7.4,7.933055555555556,14.466666666666667,...   \n",
       "96                            [0.0,635.3127777777778]   \n",
       "97  [0.0,82.16555555555556,216.66027777777776,229....   \n",
       "98  [0.0,1217.8258333333333,1223.6030555555556,125...   \n",
       "99  [0.0,12.016666666666667,21.483055555555556,640...   \n",
       "\n",
       "                                             recovery  \\\n",
       "0   [[2419.9805555555554,2437.4241666666667],[3177...   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3                                                  []   \n",
       "4                                                  []   \n",
       "..                                                ...   \n",
       "95                                                 []   \n",
       "96                                                 []   \n",
       "97                                                 []   \n",
       "98                    [[9541.77638888889,10288.5075]]   \n",
       "99                                                 []   \n",
       "\n",
       "                                                 drop  \n",
       "0                    [[3453.6875,3763.9605555555554]]  \n",
       "1                    [[13284.465,19439.800555555557]]  \n",
       "2                                       [[0.0,42.75]]  \n",
       "3                          [[0.0,491.98305555555555]]  \n",
       "4             [[4920.376666666667,6208.231666666667]]  \n",
       "..                                                ...  \n",
       "95                                                 []  \n",
       "96                          [[0.0,635.3127777777778]]  \n",
       "97                                                 []  \n",
       "98  [[10339.343055555555,10739.613055555556],[1311...  \n",
       "99                                                 []  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv('ground_truth.csv', sep = ';')\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100  \n",
    "BATCH_SIZE = 32\n",
    "NOISE_FACTOR = 0.05\n",
    "TRAIN_DIR = \"../train_reduced/\"\n",
    "CHECKPOINT_PATH = '../../models/checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(file_path: str, sequence_length = SEQUENCE_LENGTH, noise_factor = NOISE_FACTOR):\n",
    "    df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])  \n",
    "    \n",
    "    if df.empty or \"pressure\" not in df:\n",
    "        return torch.empty(0), torch.empty(0) \n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"pressure\"] = scaler.fit_transform(df[[\"pressure\"]]) \n",
    "\n",
    "    sequences, noisy_sequences = [], []\n",
    "    \n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df[\"pressure\"].iloc[i : i + sequence_length].values\n",
    "        noisy_seq = seq + noise_factor * np.random.normal(0, 1, seq.shape) \n",
    "        \n",
    "        sequences.append(seq)\n",
    "        noisy_sequences.append(noisy_seq)\n",
    "\n",
    "    return torch.tensor(noisy_sequences, dtype=torch.float32).unsqueeze(-1), torch.tensor(sequences, dtype=torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTypeDataset(Dataset):\n",
    "    def __init__(self, file_paths, sequence_length = SEQUENCE_LENGTH, noise_factor = NOISE_FACTOR):\n",
    "        self.noisy_data, self.clean_data = [], []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if os.path.exists(file_path): \n",
    "                noisy, clean = load_raw_data(file_path, sequence_length, noise_factor)\n",
    "\n",
    "                if noisy.shape[0] > 1 and clean.shape[0] > 1:  \n",
    "                    noisy = noisy.unsqueeze(-1) if noisy.dim() == 2 else noisy\n",
    "                    clean = clean.unsqueeze(-1) if clean.dim() == 2 else clean\n",
    "\n",
    "                    self.noisy_data.append(noisy)\n",
    "                    self.clean_data.append(clean)\n",
    "\n",
    "        self.noisy_data = torch.cat(self.noisy_data, dim=0)  \n",
    "        self.clean_data = torch.cat(self.clean_data, dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.noisy_data[idx], self.clean_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, epsilon = 1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return torch.mean(torch.sqrt((x - y) ** 2 + self.epsilon ** 2))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim = dim, num_heads = 4, batch_first = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, input_dim = 1, hidden_dim = 64, bottleneck_dim = 32):\n",
    "        super(DAE, self).__init__()  \n",
    "      \n",
    "        self.conv1 = nn.Conv1d(in_channels = input_dim, out_channels = 32, kernel_size = 5, padding = 2)\n",
    "        self.lstm1 = nn.LSTM(input_size = 32, hidden_size = hidden_dim, batch_first = True, bidirectional = False)\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        self.bottleneck = nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_dim, hidden_size = hidden_dim, batch_first = True, bidirectional = False)\n",
    "        self.conv2 = nn.Conv1d(in_channels = hidden_dim, out_channels = input_dim, kernel_size = 5, padding = 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.transpose(1, 2))\n",
    "        x = F.relu(x).transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.conv2(x.transpose(1, 2)).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAE_train(model, dataloader, epochs = 50, lr = 1e-3, device='cuda', save_path = \"../../models/dae_checkpoint.pth\", final_save_path = \"../../models/dae_final.pth\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = CharbonnierLoss()\n",
    "\n",
    "    best_loss = float(\"inf\")  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1} / {epochs}\") \n",
    "        \n",
    "        for noisy, clean in progress_bar:\n",
    "            noisy, clean = noisy.to(device), clean.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(noisy)\n",
    "            loss = criterion(output, clean)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1} / {epochs}, Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), save_path)  \n",
    "            print(f\"Model checkpoint saved at {save_path} (Loss improved: {best_loss:.6f})\")\n",
    "\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved at {final_save_path}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(TRAIN_DIR, file) for file in label_df['file'][:100] if file in os.listdir(TRAIN_DIR)]\n",
    "\n",
    "dataset = TorchTypeDataset(file_paths)\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noisy_batch, clean_batch in dataloader:\n",
    "    print(\"Noisy batch shape:\", noisy_batch.shape, end = ' ')  \n",
    "    print(\"Clean batch shape:\", clean_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model = DAE()\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    dae_model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "    print(f\"Checkpoint loaded from {CHECKPOINT_PATH}\")\n",
    "\n",
    "trained_model = DAE_train(dae_model, dataloader, epochs = 20, lr=1e-3, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
