{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from hmmlearn import hmm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "HMM_COMPONENTS = 3 \n",
    "\n",
    "TRAIN_DIR = \"../src/train_denoised/\"\n",
    "MODEL_HMM_PATH = \"../models/hmm_model.pkl\"\n",
    "MODEL_RNN_PATH = \"../models/rnn_model.pth\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.75, device=0) \n",
    "    \n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmm(directory, n_components=HMM_COMPONENTS):\n",
    "    all_sequences = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "        if df.empty or \"pressure\" not in df:\n",
    "            continue\n",
    "\n",
    "        all_sequences.append(df[\"pressure\"].values.reshape(-1, 1))\n",
    "\n",
    "    X = np.concatenate(all_sequences, axis=0) \n",
    "\n",
    "    hmm_model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100)\n",
    "    hmm_model.fit(X)  \n",
    "    joblib.dump(hmm_model, MODEL_HMM_PATH) \n",
    "    print(f\"HMM Model saved at {MODEL_HMM_PATH}\")\n",
    "\n",
    "    return hmm_model\n",
    "\n",
    "def label_data_with_hmm(directory, hmm_model):\n",
    "    labeled_data = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "        if df.empty or \"pressure\" not in df:\n",
    "            continue\n",
    "        \n",
    "        X = df[\"pressure\"].values.reshape(-1, 1)\n",
    "\n",
    "        states = hmm_model.predict(X) \n",
    "        df[\"state\"] = states\n",
    "\n",
    "        labeled_data.append(df)\n",
    "\n",
    "    return labeled_data\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, labeled_data):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for df in labeled_data:\n",
    "            for i in range(len(df) - SEQUENCE_LENGTH):\n",
    "                seq = df[\"pressure\"].iloc[i : i + SEQUENCE_LENGTH].values.astype(np.float32)\n",
    "                label = df[\"state\"].iloc[i + SEQUENCE_LENGTH] \n",
    "\n",
    "                self.data.append(seq)\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.data = np.array(self.data).reshape(-1, SEQUENCE_LENGTH, 1)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, output_dim=HMM_COMPONENTS):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = RNNModel().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_RNN_PATH)\n",
    "    print(f\"RNN Model saved at {MODEL_RNN_PATH}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hmm_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain_hmm\u001b[0;34m(directory, n_components)\u001b[0m\n\u001b[1;32m     12\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_sequences, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m     14\u001b[0m hmm_model \u001b[38;5;241m=\u001b[39m hmm\u001b[38;5;241m.\u001b[39mGaussianHMM(n_components\u001b[38;5;241m=\u001b[39mn_components, covariance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mhmm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(hmm_model, MODEL_HMM_PATH) \n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHMM Model saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_HMM_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hmmlearn/base.py:485\u001b[0m, in \u001b[0;36m_AbstractHMM.fit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor_\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter):\n\u001b[0;32m--> 485\u001b[0m     stats, curr_logprob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_estep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;66;03m# Compute lower bound before updating model parameters\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(curr_logprob)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hmmlearn/base.py:768\u001b[0m, in \u001b[0;36m_AbstractHMM._do_estep\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    764\u001b[0m     lattice, logprob, posteriors, fwdlattice, bwdlattice \u001b[38;5;241m=\u001b[39m impl(sub_X)\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# Derived HMM classes will implement the following method to\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# update their probability distributions, so keep\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# a single call to this method for simplicity.\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accumulate_sufficient_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m     curr_logprob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logprob\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stats, curr_logprob\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hmmlearn/_emissions.py:145\u001b[0m, in \u001b[0;36mBaseGaussianHMM._accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_accumulate_sufficient_statistics\u001b[39m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m, stats, X, lattice, posteriors, fwdlattice, bwdlattice):\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accumulate_sufficient_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbwdlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_sufficient_statistics_for_mean():\n\u001b[1;32m    152\u001b[0m         stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m posteriors\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hmmlearn/base.py:702\u001b[0m, in \u001b[0;36m_AbstractHMM._accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03mUpdate sufficient statistics from a given sample.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    forward and backward probabilities.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    697\u001b[0m impl \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_sufficient_statistics_scaling,\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_sufficient_statistics_log,\n\u001b[1;32m    700\u001b[0m }[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimplementation]\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbwdlattice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/hmmlearn/base.py:739\u001b[0m, in \u001b[0;36m_AbstractHMM._accumulate_sufficient_statistics_log\u001b[0;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m log_xi_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_hmmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_log_xi_sum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmat_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    742\u001b[0m     stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(log_xi_sum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm_model = train_hmm(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = label_data_with_hmm(TRAIN_DIR, hmm_model)\n",
    "train_dataset = TimeSeriesDataset(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = train_rnn(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
