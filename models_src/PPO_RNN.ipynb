{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.75, device=0) \n",
    "    \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 64\n",
    "N_EPISODES = 500\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_DIR = \"../src/train_denoised/\"\n",
    "TEST_DIR = \"../src/test_denoised/\"\n",
    "MODEL_SAVE_PATH = \"../models/ppo_rnn_model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Структура модели & обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.files = [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "        self.data = []\n",
    "\n",
    "        for file_path in self.files:\n",
    "            df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "            if df.empty or \"pressure\" not in df:\n",
    "                continue\n",
    "            \n",
    "            for i in range(len(df) - SEQUENCE_LENGTH):\n",
    "                self.data.append(df[\"pressure\"].iloc[i: i + SEQUENCE_LENGTH].values.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(TRAIN_DIR)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, hidden_dim=128):\n",
    "        super(RNNFeatureExtractor, self).__init__(observation_space, features_dim=hidden_dim)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  \n",
    "        return x\n",
    "\n",
    "class TimeSeriesEnv(gym.Env):\n",
    "    def __init__(self, dataloader):\n",
    "        super(TimeSeriesEnv, self).__init__()\n",
    "        self.dataloader = iter(dataloader)\n",
    "        self.current_batch = next(self.dataloader)\n",
    "        self.batch_idx = 0  \n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(SEQUENCE_LENGTH, 1), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  \n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            self.current_batch = next(self.dataloader)  \n",
    "        except StopIteration:\n",
    "            self.dataloader = iter(train_dataloader)  \n",
    "            self.current_batch = next(self.dataloader)\n",
    "\n",
    "        self.batch_idx = 0  \n",
    "        return self.current_batch[self.batch_idx].reshape(SEQUENCE_LENGTH, 1)  \n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.uniform(-1, 1)  \n",
    "        self.batch_idx += 1  \n",
    "\n",
    "        if self.batch_idx >= BATCH_SIZE:  \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        obs = self.current_batch[self.batch_idx].reshape(SEQUENCE_LENGTH, 1) if not done else self.reset()\n",
    "        return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = DummyVecEnv([lambda: TimeSeriesEnv(train_dataloader)])\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=RNNFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(hidden_dim=128),\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_env,\n",
    "    learning_rate=2.5e-4, \n",
    "    n_epochs=3,  \n",
    "    ent_coef=0.01,  \n",
    "    target_kl=0.005,  \n",
    "    clip_range=0.15, \n",
    "    verbose=1,\n",
    "    device=device,\n",
    "    policy_kwargs=policy_kwargs\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=N_EPISODES * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка предсказаний, формирование submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_INPUT_PATH = \"../src/raw_data/submit.csv\"\n",
    "SUBMIT_OUTPUT_PATH = \"../src/submit.csv\"\n",
    "MODEL_SAVE_PATH = \"../models/ppo_transformer_model.pkl\"\n",
    "TEST_DIR = \"../src/test_denoised/\"\n",
    "SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PPO.load(MODEL_SAVE_PATH, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intervals(predictions, time_series):\n",
    "    intervals = []\n",
    "    start = None\n",
    "\n",
    "    for i in range(len(predictions) - 1):\n",
    "        if predictions[i] == predictions[i + 1]:  \n",
    "            if start is None:  \n",
    "                start = time_series[i]\n",
    "        else:\n",
    "            if start is not None:\n",
    "                end = time_series[i]\n",
    "                intervals.append([start, end])\n",
    "                start = None  \n",
    "\n",
    "    if start is not None:\n",
    "        intervals.append([start, time_series[-1]])\n",
    "\n",
    "    return intervals\n",
    "\n",
    "submit_df = pd.read_csv(SUBMIT_INPUT_PATH, names=[\"file\", \"recovery\", \"drop\"])\n",
    "\n",
    "predicted_dfs = []\n",
    "results = []\n",
    "\n",
    "for _, row in submit_df.iterrows():\n",
    "    file_name = row[\"file\"]\n",
    "    file_path = os.path.join(TEST_DIR, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_name} not found\")\n",
    "        results.append([file_name, [], []])\n",
    "        continue\n",
    "\n",
    "    df_test = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "\n",
    "    test_sequences = np.array([\n",
    "        df_test[\"pressure\"].iloc[i: i + SEQUENCE_LENGTH].values.reshape(SEQUENCE_LENGTH, 1)\n",
    "        for i in range(len(df_test) - SEQUENCE_LENGTH)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    all_predictions = [[] for _ in range(len(df_test))]\n",
    "\n",
    "    for start in range(len(test_sequences)):\n",
    "        obs = test_sequences[start][None, :, :] \n",
    "        pred_class, _ = loaded_model.predict(obs, deterministic=True)\n",
    "\n",
    "        for i in range(SEQUENCE_LENGTH):\n",
    "            if start + i < len(df_test):\n",
    "                all_predictions[start + i].append(pred_class)\n",
    "\n",
    "\n",
    "    final_classes = np.zeros(len(df_test))\n",
    "\n",
    "    for i in range(len(all_predictions)):\n",
    "        if all_predictions[i]:  \n",
    "            final_classes[i] = Counter(all_predictions[i]).most_common(1)[0][0]  \n",
    "        elif i > 0:  \n",
    "            final_classes[i] = final_classes[i - 1]  \n",
    "\n",
    "    df_test[\"prediction\"] = final_classes\n",
    "    predicted_dfs.append(df_test)\n",
    "\n",
    "    recovery_intervals = extract_intervals(final_classes == 2, df_test[\"time\"].values)\n",
    "    drop_intervals = extract_intervals(final_classes == 1, df_test[\"time\"].values)\n",
    "\n",
    "    results.append([file_name, recovery_intervals, drop_intervals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_output_df = pd.DataFrame(results, columns=[\"file\", \"recovery\", \"drop\"])\n",
    "submit_output_df.to_csv(SUBMIT_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "дальше нужно проанализировать predicted_dfs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
