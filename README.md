# Siam ML Hack 2025


## Постановка задачи
Разработка решения для определения наличия и границ полезных данных для анализа в результатах гидродинамических исследований скважин

---

## Описание задачи 

Участникам предоставляется набор данных, содержащий временные ряды, характеризующие изменение давления нефтяных скважин в процессе гидродинамических исследований. 

Необходимо разработать модель для бинарной классификации фреймов на предмет наличия полезных данных для ГДИС с точным определением начала и конца полезных участков данных.

---

## Данные

Данные предоставлены в виде текстовых файлов, содержащих временные ряды. Каждая новая строка - новая запись. Первый столбец содержит метки времени, второй - метки давления в скважение

`src/raw_data/ground_truth.csv` - файл с разметкой некоторых файлов из обучающей выборки
`src/raw_data/test` - тестовая выборка
`src/raw_data/task_21.zip` - обучающая выборка
`src/raw_data/train_reduced.zip` - обучающая выборка с пониженной плотностью
`src/raw_data/train_denoised.zip` - обучающая выборка с пониженной плотность, прошедшая процесс обработки шумов
`src/raw_data/test_denoised.zip` - тестовая выборка с пониженной плотностью, прошедшая процесс обработки шумов

---

## Формат ответа

Итоговый ответ должен быть представлен в файле `submit.csv`, где каждому названию файла соответствуют массивы с размеченными паттернами. 

`src/raw_data/submit.csv` - шаблон файла с загрузкой ответа
`src/submits/submit.csv` - лучший ответ 

---

## Используемые методы

1. **Предобработка данных**
    - Нормализация данных (MinMax Scaling) - приведение данных давления к единому масштабу.

    - Сглаживание шумов (Denoising Autoencoder).

2. **Выделение паттернов (КВД, КПД)**
    - Кластеризация (DBSCAN) - выявление трендов изменения давления и сегментация данных.

    - Объединение схожих сегментов, фильтрация малозначимых участков.

3. **Постобработка данных**
    - Анализ градиентов давления – определение направления изменения давления для уточнения границ КВД и КПД.

    - Фильтрация мелких и неинформативных кластеров – устранение ложных сегментов.

    - Удаление выбросов и аномальных скачков – исключение технических шумов, не относящихся к паттернам.

---

## Структура проекта

Проект организован в виде модульной структуры, включающей основные компоненты для обработки временных рядов давления, выделения паттернов и обучения моделей машинного обучения.

* models/ – Содержит обученные модели.

* models_src/ – Исходный код для обучения и доработки моделей.

* notebooks/ – Jupyter-ноутбуки для анализа данных, визуализации и тестирования алгоритмов.

* src/ – Директория с данными проекта

* utils/ – Вспомогательные функции для предобработки и загрузки данных.

* .gitattributes/ – Файлы конфигурации Git.

* .gitignore/ – Исключенные файлы, включая обработанные данные.

* requirements.txt – Список зависимостей и библиотек, необходимых для работы проекта.

---

## Инференс

1) **Подготовка данных**
    * Для проверки работы итогового алгоритма можно использовать уже предобработанные тестовые данные src/raw_data/train_denoised.zip. Архив необходимо вручную распаковать в директорию src/train_denoised или использовать для этого скрипт utils/unziper3.py.
    * Для использования других данных, необходимо запустить utils/denoiser.py, указав корректные пути в конце файла (process_directory("директория_с_вашими_данными", DENOISED_TEST_ZIP), текущие вызовы process_directory удалить). Если вы не меняли DENOISED_TEST_ZIP, то архив с обработанными данными будет сохранен в                 src/raw_data/train_denoised.zip, его необходимо вручную распаковать в директорию src/train_denoised или использовать для этого скрипт utils/unziper3.py.

2) **Запуск модели**
    * Запустить models_src/DBScan.ipynb. Будет сформирован csv файл с результатами определения необходимых паттернов. В конце файла можно визуализировать результаты.

Такое разбиение на файлы сделано для удобства разработки и масштабирования, для упрощения использования конечной модели можно объединить все этапы работы с входными данными в один файл. 
