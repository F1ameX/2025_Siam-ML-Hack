{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.set_per_process_memory_fraction(0.7, device = 0)\n",
    "os.chdir('../src/raw_data')\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>mark</th>\n",
       "      <th>recovery</th>\n",
       "      <th>drop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00e03657-8e1e-4c8c-a724-1d3c77b48510</td>\n",
       "      <td>[0.0,235.9225,237.06666666666666,2076.06055555...</td>\n",
       "      <td>[[2419.9805555555554,2437.4241666666667],[3177...</td>\n",
       "      <td>[[3453.6875,3763.9605555555554]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00e4dba2-36d2-42b4-beb1-c55aed75f506</td>\n",
       "      <td>[0.0,7979.234444444444,13284.465,19439.8005555...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[13284.465,19439.800555555557]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00f035b7-ad7a-4f30-9081-522a3c10805b</td>\n",
       "      <td>[0.0,42.75,2438.3330555555553]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0,42.75]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01a0c034-6afc-4e73-95fa-621f702a0b7d</td>\n",
       "      <td>[0.0,491.98305555555555,1439.9830555555557,154...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0,491.98305555555555]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01a530d3-6496-4515-9fbb-4f44e298fd29</td>\n",
       "      <td>[0.0,1287.0341666666666,1288.0483333333334,156...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[4920.376666666667,6208.231666666667]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1dfaf03c-e297-4d92-a0bf-40b1a829391f</td>\n",
       "      <td>[0.0,7.4,7.933055555555556,14.466666666666667,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1e149fbd-41c6-4779-b87d-c5dc17fbb4c0</td>\n",
       "      <td>[0.0,635.3127777777778]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0,635.3127777777778]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1e19b77c-8a0e-4749-a384-9c1e679035bf</td>\n",
       "      <td>[0.0,82.16555555555556,216.66027777777776,229....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1e4b4c18-1e32-45eb-917a-5760e33fbaca</td>\n",
       "      <td>[0.0,1217.8258333333333,1223.6030555555556,125...</td>\n",
       "      <td>[[9541.77638888889,10288.5075]]</td>\n",
       "      <td>[[10339.343055555555,10739.613055555556],[1311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1e7f7ecb-a6a7-40ef-9d1c-48aa96eb6c38</td>\n",
       "      <td>[0.0,12.016666666666667,21.483055555555556,640...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file  \\\n",
       "0   00e03657-8e1e-4c8c-a724-1d3c77b48510   \n",
       "1   00e4dba2-36d2-42b4-beb1-c55aed75f506   \n",
       "2   00f035b7-ad7a-4f30-9081-522a3c10805b   \n",
       "3   01a0c034-6afc-4e73-95fa-621f702a0b7d   \n",
       "4   01a530d3-6496-4515-9fbb-4f44e298fd29   \n",
       "..                                   ...   \n",
       "95  1dfaf03c-e297-4d92-a0bf-40b1a829391f   \n",
       "96  1e149fbd-41c6-4779-b87d-c5dc17fbb4c0   \n",
       "97  1e19b77c-8a0e-4749-a384-9c1e679035bf   \n",
       "98  1e4b4c18-1e32-45eb-917a-5760e33fbaca   \n",
       "99  1e7f7ecb-a6a7-40ef-9d1c-48aa96eb6c38   \n",
       "\n",
       "                                                 mark  \\\n",
       "0   [0.0,235.9225,237.06666666666666,2076.06055555...   \n",
       "1   [0.0,7979.234444444444,13284.465,19439.8005555...   \n",
       "2                      [0.0,42.75,2438.3330555555553]   \n",
       "3   [0.0,491.98305555555555,1439.9830555555557,154...   \n",
       "4   [0.0,1287.0341666666666,1288.0483333333334,156...   \n",
       "..                                                ...   \n",
       "95  [0.0,7.4,7.933055555555556,14.466666666666667,...   \n",
       "96                            [0.0,635.3127777777778]   \n",
       "97  [0.0,82.16555555555556,216.66027777777776,229....   \n",
       "98  [0.0,1217.8258333333333,1223.6030555555556,125...   \n",
       "99  [0.0,12.016666666666667,21.483055555555556,640...   \n",
       "\n",
       "                                             recovery  \\\n",
       "0   [[2419.9805555555554,2437.4241666666667],[3177...   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3                                                  []   \n",
       "4                                                  []   \n",
       "..                                                ...   \n",
       "95                                                 []   \n",
       "96                                                 []   \n",
       "97                                                 []   \n",
       "98                    [[9541.77638888889,10288.5075]]   \n",
       "99                                                 []   \n",
       "\n",
       "                                                 drop  \n",
       "0                    [[3453.6875,3763.9605555555554]]  \n",
       "1                    [[13284.465,19439.800555555557]]  \n",
       "2                                       [[0.0,42.75]]  \n",
       "3                          [[0.0,491.98305555555555]]  \n",
       "4             [[4920.376666666667,6208.231666666667]]  \n",
       "..                                                ...  \n",
       "95                                                 []  \n",
       "96                          [[0.0,635.3127777777778]]  \n",
       "97                                                 []  \n",
       "98  [[10339.343055555555,10739.613055555556],[1311...  \n",
       "99                                                 []  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv('ground_truth.csv', sep = ';')\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100  \n",
    "BATCH_SIZE = 32\n",
    "NOISE_FACTOR = 0.05\n",
    "TRAIN_DIR = \"../train_reduced/\"\n",
    "CHECKPOINT_PATH = '../../models/checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(file_path: str, sequence_length = SEQUENCE_LENGTH, noise_factor = NOISE_FACTOR):\n",
    "    df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])  \n",
    "    \n",
    "    if df.empty or \"pressure\" not in df:\n",
    "        return torch.empty(0), torch.empty(0) \n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"pressure\"] = scaler.fit_transform(df[[\"pressure\"]]) \n",
    "\n",
    "    sequences, noisy_sequences = [], []\n",
    "    \n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df[\"pressure\"].iloc[i : i + sequence_length].values\n",
    "        noisy_seq = seq + noise_factor * np.random.normal(0, 1, seq.shape) \n",
    "        \n",
    "        sequences.append(seq)\n",
    "        noisy_sequences.append(noisy_seq)\n",
    "\n",
    "    return torch.tensor(noisy_sequences, dtype=torch.float32).unsqueeze(-1), torch.tensor(sequences, dtype=torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTypeDataset(Dataset):\n",
    "    def __init__(self, file_paths, sequence_length = SEQUENCE_LENGTH, noise_factor = NOISE_FACTOR):\n",
    "        self.noisy_data, self.clean_data = [], []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if os.path.exists(file_path): \n",
    "                noisy, clean = load_raw_data(file_path, sequence_length, noise_factor)\n",
    "\n",
    "                if noisy.shape[0] > 1 and clean.shape[0] > 1:  \n",
    "                    noisy = noisy.unsqueeze(-1) if noisy.dim() == 2 else noisy\n",
    "                    clean = clean.unsqueeze(-1) if clean.dim() == 2 else clean\n",
    "\n",
    "                    self.noisy_data.append(noisy)\n",
    "                    self.clean_data.append(clean)\n",
    "\n",
    "        self.noisy_data = torch.cat(self.noisy_data, dim=0)  \n",
    "        self.clean_data = torch.cat(self.clean_data, dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.noisy_data[idx], self.clean_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, epsilon = 1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return torch.mean(torch.sqrt((x - y) ** 2 + self.epsilon ** 2))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim = dim, num_heads = 4, batch_first = True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, input_dim = 1, hidden_dim = 64, bottleneck_dim = 32):\n",
    "        super(DAE, self).__init__()  \n",
    "      \n",
    "        self.conv1 = nn.Conv1d(in_channels = input_dim, out_channels = 32, kernel_size = 5, padding = 2)\n",
    "        self.lstm1 = nn.LSTM(input_size = 32, hidden_size = hidden_dim, batch_first = True, bidirectional = False)\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        self.bottleneck = nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_dim, hidden_size = hidden_dim, batch_first = True, bidirectional = False)\n",
    "        self.conv2 = nn.Conv1d(in_channels = hidden_dim, out_channels = input_dim, kernel_size = 5, padding = 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.transpose(1, 2))\n",
    "        x = F.relu(x).transpose(1, 2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.conv2(x.transpose(1, 2)).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAE_train(model, dataloader, epochs = 50, lr = 1e-3, device='cuda', save_path = \"../../models/dae_checkpoint.pth\", final_save_path = \"../../models/dae_final.pth\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = CharbonnierLoss()\n",
    "\n",
    "    best_loss = float(\"inf\")  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1} / {epochs}\") \n",
    "        \n",
    "        for noisy, clean in progress_bar:\n",
    "            noisy, clean = noisy.to(device), clean.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(noisy)\n",
    "            loss = criterion(output, clean)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1} / {epochs}, Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), save_path)  \n",
    "            print(f\"Model checkpoint saved at {save_path} (Loss improved: {best_loss:.6f})\")\n",
    "\n",
    "    torch.save(model.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved at {final_save_path}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, input_tensor, scaler, chunk_size=500, device='cuda'):\n",
    "    model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    denoised_chunks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chunk in torch.split(input_tensor, chunk_size, dim=0):\n",
    "            denoised_chunk = model(chunk)\n",
    "            denoised_chunks.append(denoised_chunk.cpu())\n",
    "    \n",
    "    denoised_output = torch.cat(denoised_chunks, dim=0)\n",
    "    return scaler.inverse_transform(denoised_output.numpy().squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(TRAIN_DIR, file) for file in label_df['file'][:100] if file in os.listdir(TRAIN_DIR)]\n",
    "\n",
    "dataset = TorchTypeDataset(file_paths)\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy batch shape: torch.Size([32, 100, 1]) Clean batch shape: torch.Size([32, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "for noisy_batch, clean_batch in dataloader:\n",
    "    print(\"Noisy batch shape:\", noisy_batch.shape, end = ' ')  \n",
    "    print(\"Clean batch shape:\", clean_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2: 100%|██████████| 13539/13539 [01:47<00:00, 125.64it/s, loss=0.0133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2, Avg Loss: 0.012767\n",
      "Model checkpoint saved at ../../models/dae_checkpoint.pth (Loss improved: 0.012767)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 2: 100%|██████████| 13539/13539 [01:48<00:00, 124.89it/s, loss=0.00882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 2, Avg Loss: 0.010475\n",
      "Model checkpoint saved at ../../models/dae_checkpoint.pth (Loss improved: 0.010475)\n",
      "Final model saved at ../../models/dae_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dae_model = DAE()\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    dae_model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "    print(f\"Checkpoint loaded from {CHECKPOINT_PATH}\")\n",
    "\n",
    "trained_model = DAE_train(dae_model, dataloader, epochs = 2, lr=1e-3, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.eval()\n",
    "trained_model.to(device)\n",
    "\n",
    "def preprocess_file(file_path, sequence_length=100):\n",
    "    df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"pressure\"] = scaler.fit_transform(df[[\"pressure\"]])\n",
    "    \n",
    "    sequences = []\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df[\"pressure\"].iloc[i : i + sequence_length].values\n",
    "        sequences.append(seq)\n",
    "\n",
    "    input_tensor = torch.tensor(np.array(sequences), dtype=torch.float32).unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "\n",
    "    print(f\"Размер `df`: {df.shape}\")\n",
    "    print(f\"Размер `sequences`: {len(sequences)}\")\n",
    "    print(f\"Размер `input_tensor`: {input_tensor.shape}\")\n",
    "\n",
    "    if \"time\" in df and len(df[\"time\"]) >= len(sequences):\n",
    "        time_values = df[\"time\"][:len(sequences)].values\n",
    "        return input_tensor, scaler, time_values\n",
    "    else:\n",
    "        return input_tensor, scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер `df`: (54215, 2)\n",
      "Размер `sequences`: 54115\n",
      "Размер `input_tensor`: torch.Size([54115, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "test_file = \"../raw_data/test/1cbce6e5-9f0b-419f-9527-7add4e255217\" \n",
    "result = preprocess_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер входа в модель: torch.Size([54115, 100, 1])\n",
      "Размер выхода из модели: torch.Size([54115, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if len(result) == 3:\n",
    "    input_tensor, scaler, time_values = result\n",
    "else:\n",
    "    input_tensor, scaler = result\n",
    "    time_values = None\n",
    "\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "chunk_size = 500  \n",
    "denoised_chunks = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for chunk in torch.split(input_tensor, chunk_size, dim=0):\n",
    "        denoised_chunk = dae_model(chunk.to(device))  \n",
    "        denoised_chunks.append(denoised_chunk.cpu()) \n",
    "\n",
    "denoised_output = torch.cat(denoised_chunks, dim=0)\n",
    "denoised_data = scaler.inverse_transform(denoised_output.numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45.770573, 45.69212 , 45.374577, ..., 46.061058, 46.190327,\n",
       "        46.160522],\n",
       "       [45.786808, 45.708652, 45.3913  , ..., 46.070038, 46.200497,\n",
       "        46.169773],\n",
       "       [45.80486 , 45.72792 , 45.409447, ..., 46.079105, 46.210453,\n",
       "        46.179783],\n",
       "       ...,\n",
       "       [27.345001, 27.270355, 27.285076, ..., 27.168692, 27.334549,\n",
       "        27.47717 ],\n",
       "       [27.344183, 27.269205, 27.28544 , ..., 27.168417, 27.334524,\n",
       "        27.477148],\n",
       "       [27.34387 , 27.2686  , 27.285686, ..., 27.166836, 27.332396,\n",
       "        27.47537 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (54115) does not match length of index (54215)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m orig_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpressure\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m orig_df\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpred_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpressure\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m denoised_data\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (54115) does not match length of index (54215)"
     ]
    }
   ],
   "source": [
    "file_path = \"../raw_data/test/1cbce6e5-9f0b-419f-9527-7add4e255217\" \n",
    "orig_df = pd.read_csv(file_path, sep=\"\\\\s+\", names=[\"time\", \"pressure\"])\n",
    "pred_df = orig_df\n",
    "pred_df['pressure'] = denoised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер orig_df: (54215, 2)\n",
      "Размер denoised_data: (5411500, 1)\n",
      "Размер first_100_tensor (до модели): torch.Size([1, 100, 1])\n",
      "Размер first_100_denoised (после модели): (100,)\n",
      "Размер first_100_denoised (после reshape): (100, 1)\n",
      "Размер denoised_data (после reshape): (5411500, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (5411600) does not match length of index (54215)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m full_denoised \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([first_100_denoised, denoised_data], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# === Добавляем в DataFrame ===\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43morig_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdenoised_pressure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m full_denoised\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shara\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (5411600) does not match length of index (54215)"
     ]
    }
   ],
   "source": [
    "# === Готовим тензор первых 100 значений ===\n",
    "first_100_tensor = torch.tensor(orig_df[\"pressure\"].values[:100], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "\n",
    "print(f\"Размер orig_df: {orig_df.shape}\")\n",
    "print(f\"Размер denoised_data: {denoised_data.shape}\")\n",
    "\n",
    "print(f\"Размер first_100_tensor (до модели): {first_100_tensor.shape}\")\n",
    "\n",
    "# === Прогоняем через модель ===\n",
    "with torch.no_grad():\n",
    "    first_100_denoised = dae_model(first_100_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "print(f\"Размер first_100_denoised (после модели): {first_100_denoised.shape}\")\n",
    "\n",
    "# === Приводим `first_100_denoised` к (100, 1) ===\n",
    "first_100_denoised = first_100_denoised.reshape(-1, 1)\n",
    "\n",
    "# === Приводим `denoised_data` к (N, 1), если это нужно ===\n",
    "denoised_data = denoised_data.reshape(-1, 1)\n",
    "\n",
    "\n",
    "print(f\"Размер first_100_denoised (после reshape): {first_100_denoised.shape}\")\n",
    "print(f\"Размер denoised_data (после reshape): {denoised_data.shape}\")\n",
    "\n",
    "# === Объединяем тензоры ===\n",
    "full_denoised = np.concatenate([first_100_denoised, denoised_data], axis=0)\n",
    "\n",
    "# === Добавляем в DataFrame ===\n",
    "orig_df[\"denoised_pressure\"] = full_denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
